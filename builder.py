import streamlit as st
from concurrent.futures import ThreadPoolExecutor
import gdown

def build_ts_links() -> list:
    """
    Constructs a list of URLs for downloading TS (Table Specification) files
    from the Nomisweb data repository for the 2021 census.

    Each TS file is named following a specific format with a zero-padded index,
    which this function dynamically constructs. The function assumes that there
    are exactly 74 TS files available, and the URLs are formed by appending
    the file indices, properly formatted, to a base URL.

    Returns:
        list of str: A list containing 74 URLs, each corresponding to a downloadable
                     TS file in ZIP format from the Nomisweb server.

    Example of output:
        ['https://www.nomisweb.co.uk/output/census/2021/census2021-ts001.zip',
         'https://www.nomisweb.co.uk/output/census/2021/census2021-ts002.zip',
         ...,
         'https://www.nomisweb.co.uk/output/census/2021/census2021-ts074.zip']
    """

    # There are 74 TS files available on Nomis Data
    # it appears 14,57,43,42,69,39 zips do not exist, skip those!
    missing_files_indices = [14, 57, 43, 42, 69, 49]
    ts_count = 74
    ts_base_link = "https://www.nomisweb.co.uk/output/census/2021/census2021"

    # Our list of data sources. Very important!
    links = []
    # Adding non-census data to download list
    # Median property value
    links.append("https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/housing/datasets/hpssadataset2medianhousepricebymsoaquarterlyrollingyear/current/hpssadataset2medianpricepaidbymsoa.zip")
    # Shapefiles from my gdrive
    links.append('https://drive.google.com/uc?id=17bm9WWVn6R9gHxLvwirCafThKJAQ9v6x')

    #Building Links
    for index in range(1,75):
        if index in missing_files_indices:
            pass
        else:
            if index < 10:
                links.append(ts_base_link+f"-ts00{index}.zip") #ts074.zip
            else:
                links.append(ts_base_link+f"-ts0{index}.zip") #ts074.zip


    return links


def download_link(link):
    """
    Downloads a file from the specified URL using the wget or gdown command via subprocess.
    We use gdown to skip authentication for publicly shared gdrives.

    This function is designed to download a single file from a given link. It uses the wget command,
    which must be installed on the system where this script is run. The output from wget is suppressed.

    Files hosted on Google Drive are downloaded using the `gdown` library, which facilitates the download of large files
    from Google Drive without needing manual authorization.

    These shapefiles are hosted on my private Google Drive because the ONS Open Geography Portal
    often is unavailable or downloads are broken.

    Args:
        link (str): The URL from which the file will be downloaded.

    Notes:
        The function does not return any value. It assumes wget and gdrive is installed on the system and added to the system's PATH.
        Any download errors or issues related to network connectivity must be handled externally.
    """
    # Use gdown for google
    if "drive.google" in link:
        gdown.download(link, quiet=True)
    # Use wget for everything else
    else:
        subprocess.run(["wget", link], stdout=subprocess.DEVNULL)

def download_all_links():
    """
    Downloads multiple files concurrently using URLs generated by the build_ts_links function.
    Incorporates a tqdm progress bar to visually monitor the download progress.

    This function fetches a list of URLs from build_ts_links() and initiates concurrent downloads using
    ThreadPoolExecutor. It is set to use 20 worker threads to perform the downloads simultaneously,
    which speeds up the process when dealing with multiple files.

    Notes:
        The function uses download_link() to perform the downloads. It does not return any value or provide
        download progress information. Ensure network and system resources are adequate to handle multiple
        concurrent downloads.

        Adjust the `max_workers` parameter based on system capabilities and network bandwidth to optimize performance
        without overwhelming system resources.
    """
def download_all_links():
    links = build_ts_links()
    progress_bar = st.progress(0)  # Streamlit progress bar initialized
    step = 1 / len(links)
    progress_value = 0

    with ThreadPoolExecutor(max_workers=10) as executor:
        # Initialize progress bar with the total number of downloads to track
        for _ in executor.map(download_link, links):
            progress_value += step
            progress_bar.progress(progress_value)

    st.success("Downloads finished!")


